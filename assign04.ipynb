{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.66.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Users/himanshi/.pyenv/versions/3.8.13/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "import os\n",
    "import shutil\n",
    "import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I_Diorite_06.jpg',\n",
       "  'I_Diorite_12.jpg',\n",
       "  'S_Breccia_09.jpg',\n",
       "  'I_Obsidian_03.jpg',\n",
       "  'I_Basalt_01.jpg',\n",
       "  'M_Quartzite_05.jpg',\n",
       "  'M_Quartzite_11.jpg',\n",
       "  'I_Pumice_07.jpg',\n",
       "  'S_Sandstone_03.jpg',\n",
       "  'S_Bituminous Coal_06.jpg'],\n",
       " ['I_Diorite_2_120.jpg',\n",
       "  'S_Rock Gypsum_1_120.jpg',\n",
       "  'S_Conglomerate_2_120.jpg',\n",
       "  'M_Slate_1_120.jpg',\n",
       "  'M_Migmatite_2_120.jpg',\n",
       "  'S_Dolomite_2_120.jpg',\n",
       "  'M_Hornfels_1_120.jpg',\n",
       "  'M_Schist_3_120.jpg',\n",
       "  'I_Andesite_1_120.jpg',\n",
       "  'I_Pegmatite_4_120.jpg'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "import os\n",
    "\n",
    "# Path to the uploaded zip files\n",
    "zip_path_360 = '360 Rocks.zip'\n",
    "zip_path_120 = '120 Rocks.zip'\n",
    "\n",
    "# Directory to extract the contents\n",
    "extract_dir_360 = '360 Rocks'\n",
    "extract_dir_120 = '120 Rocks'\n",
    "\n",
    "# Extract the files\n",
    "with ZipFile(zip_path_360, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir_360)\n",
    "\n",
    "with ZipFile(zip_path_120, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir_120)\n",
    "\n",
    "# List the first few files in each directory to understand their structure\n",
    "files_360 = os.listdir(extract_dir_360)[:10]\n",
    "files_120 = os.listdir(extract_dir_120)[:10]\n",
    "\n",
    "files_360, files_120\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Basalt', 'Migmatite', 'Marble', 'Micrite', 'Granite']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import shutil\n",
    "\n",
    "# Directory for organized training and validation datasets\n",
    "training_dir = 'Training Dataset'\n",
    "validation_dir = 'Validation Dataset'\n",
    "\n",
    "# Create directories if they do not exist\n",
    "os.makedirs(training_dir, exist_ok=True)\n",
    "os.makedirs(validation_dir, exist_ok=True)\n",
    "\n",
    "# Function to group images by rock type\n",
    "def group_images_by_rock_type(files, directory):\n",
    "    rock_groups = defaultdict(list)\n",
    "    for file in files:\n",
    "        rock_type = file.split('_')[1]  # Assuming format is {Type}_{RockType}_{Number}.jpg\n",
    "        rock_groups[rock_type].append(os.path.join(directory, file))\n",
    "    return rock_groups\n",
    "\n",
    "# List all files in the '360 Rocks' directory\n",
    "all_files_360 = os.listdir(extract_dir_360)\n",
    "rock_groups_360 = group_images_by_rock_type(all_files_360, extract_dir_360)\n",
    "\n",
    "# Select 12 images per rock type for the training dataset\n",
    "for rock_type, file_list in rock_groups_360.items():\n",
    "    selected_files = file_list[:12]  # Take the first 12 images\n",
    "    rock_type_dir = os.path.join(training_dir, rock_type)\n",
    "    os.makedirs(rock_type_dir, exist_ok=True)\n",
    "    for file_path in selected_files:\n",
    "        shutil.copy(file_path, rock_type_dir)\n",
    "\n",
    "# List the first few directories to confirm creation and population\n",
    "os.listdir(training_dir)[:5]  # Display some of the rock type directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Basalt', 'Migmatite', 'Marble', 'Micrite', 'Granite']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all files in the '120 Rocks' directory\n",
    "all_files_120 = os.listdir(extract_dir_120)\n",
    "rock_groups_120 = group_images_by_rock_type(all_files_120, extract_dir_120)\n",
    "\n",
    "# Select 4 images per rock type for the validation dataset\n",
    "for rock_type, file_list in rock_groups_120.items():\n",
    "    selected_files = file_list[:4]  # Take the first 4 images\n",
    "    rock_type_dir = os.path.join(validation_dir, rock_type)\n",
    "    os.makedirs(rock_type_dir, exist_ok=True)\n",
    "    for file_path in selected_files:\n",
    "        shutil.copy(file_path, rock_type_dir)\n",
    "\n",
    "# List the first few directories to confirm creation and population\n",
    "os.listdir(validation_dir)[:5]  # Display some of the rock type directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total augmented images created: 3600\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "# Define more robust transformations with a wider range of randomness\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.RandomRotation(degrees=(0, 90)),  # Wider range of rotation\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # 50% chance of horizontal flip\n",
    "    transforms.RandomVerticalFlip(p=0.5),  # 50% chance of vertical flip\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.RandomResizedCrop(size=(224, 224), scale=(0.7, 1.0)),  # Random crops\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Assuming train_folder is the path to the training dataset\n",
    "train_folder = 'Training Dataset'  # Replace with your actual path to the training data\n",
    "train_dataset = ImageFolder(train_folder, transform=data_transforms)\n",
    "\n",
    "# Updated function to generate and save varied augmentations\n",
    "def save_augmented_images(dataset, save_folder, num_augmented_images):\n",
    "    os.makedirs(save_folder, exist_ok=True)\n",
    "    total_images = 0\n",
    "\n",
    "    for i, (image, label) in enumerate(dataset):\n",
    "        category_folder = os.path.join(save_folder, dataset.classes[label])\n",
    "        os.makedirs(category_folder, exist_ok=True)\n",
    "\n",
    "        for j in range(num_augmented_images):\n",
    "            # Apply additional transformations for each augmentation\n",
    "            extra_transforms = transforms.Compose([\n",
    "                transforms.RandomRotation(degrees=random.randint(0, 90)),\n",
    "                transforms.ColorJitter(\n",
    "                    brightness=random.uniform(0.1, 0.3),\n",
    "                    contrast=random.uniform(0.1, 0.3),\n",
    "                    saturation=random.uniform(0.1, 0.3),\n",
    "                    hue=random.uniform(0.1, 0.3),\n",
    "                ),\n",
    "                transforms.ToPILImage(),  # Convert tensor to PIL Image for saving\n",
    "            ])\n",
    "            augmented_image = extra_transforms(image)  # Apply the extra transformations\n",
    "            augmented_image_path = os.path.join(category_folder, f\"augmented_{i}_{j}.jpeg\")\n",
    "            augmented_image.save(augmented_image_path)  # Save the image\n",
    "            total_images += 1\n",
    "\n",
    "    print(f\"Total augmented images created: {total_images}\")\n",
    "\n",
    "# Path where augmented images will be saved\n",
    "save_augmented_images_path = \"train_aug\"  # Adjust as needed\n",
    "save_augmented_images(train_dataset, save_augmented_images_path, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_folder = 'train_aug'\n",
    "val_folder = \"Validation Dataset\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train = ImageFolder(train_folder, transform=transform)\n",
    "val = ImageFolder(val_folder, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val, batch_size=512, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/himanshi/.pyenv/versions/3.8.13/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/himanshi/.pyenv/versions/3.8.13/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 8/8 [01:31<00:00, 11.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 1, Loss - 3.3970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [01:30<00:00, 11.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch - 2, Loss - 3.3660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 4/8 [00:50<00:49, 12.38s/it]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import tqdm\n",
    "\n",
    "# Assuming 'device', 'train_loader', 'val_loader' are already defined\n",
    "\n",
    "# Load pre-trained VGG16 model\n",
    "model = torchvision.models.vgg16(pretrained=True)\n",
    "\n",
    "# Freeze the parameters in the feature extractor part\n",
    "for param in model.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Redefining the classifier part of VGG16\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(25088, 4096),     # Original VGG first classifier layer size\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),            # Dropout for regularization\n",
    "    nn.Linear(4096, 1024),      # Additional dense layer\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(1024, 8),         # Narrowing down before final layer\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8, 30),           # Final layer with 30 outputs\n",
    "    nn.Softmax(dim=1)           # Softmax at the end for classification\n",
    ")\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (Optimize only the classifier parameters, feature layers are frozen)\n",
    "optimizer = optim.Adam(model.classifier.parameters(), lr=0.0001)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    losses = []\n",
    "    for batch_idx, (data, target) in tqdm.tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    scheduler.step()  # Adjust learning rate\n",
    "    \n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    print(f'Epoch - {epoch + 1}, Loss - {avg_loss:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in val_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the network on the validation images: {accuracy:.2f}%')\n",
    "\n",
    "# Save the trained model weights\n",
    "torch.save(model.state_dict(), 'model_vgg16.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
